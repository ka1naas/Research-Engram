'''
åŠŸèƒ½aï¼šå¯¹ä¸Šä¼ çš„è®ºæ–‡ç”Ÿæˆæ‘˜è¦ï¼Œä»¥åŠå¯¹æŠ—æ€§æœç´¢éœ€è¦çš„å†…å®¹
åŠŸèƒ½bï¼šå¸®ç”¨æˆ·ç”Ÿæˆæˆ–æ”¹è¿›ideaï¼Œæˆ–è€…ï¼Œè¿›è¡Œæ™®é€šå¯¹è¯ï¼Œå››ç§æ¨¡å¼
åŠŸèƒ½cï¼šåœ¨ç”¨æˆ·å·²ç»æœ‰ideaçš„æƒ…å†µä¸‹ï¼Œéœ€è¦ä¸€ç‚¹æ‰¹è¯„æ—¶æ‰é‡‡ç”¨çš„
åç»­æ›´æ–°è®¡åˆ’ï¼šåŠ å…¥çœŸæ­£çš„function callingå®ç°ç®€å•çš„agentä»»åŠ¡
'''
import os
from sqlalchemy.orm import Session
from pypdf import PdfReader
from fastapi import UploadFile
import schemas, crud
from vector_memory import VectorMemory
import datetime
import openai
from dotenv import load_dotenv
import models
import utils
import json
import re

# è¯»å– .env
load_dotenv()

# åˆå§‹åŒ– DeepSeek
client = openai.OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"), # ä»ç³»ç»Ÿç¯å¢ƒå˜é‡é‡Œæ‹¿é’¥åŒ™ï¼Œè€Œä¸æ˜¯å†™æ­»åœ¨ä»£ç é‡Œ
    base_url="https://api.deepseek.com"
)

# åˆå§‹åŒ–å‘é‡è®°å¿†åº“ï¼ˆå•ä¾‹æ¨¡å¼ï¼šæ•´ä¸ªç³»ç»Ÿåªç”¨è¿™ä¸€ä¸ªå®ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹ï¼‰
# æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å‡è®¾ vector_memory.py åœ¨åŒä¸€ç›®å½•ä¸‹
memory_core = VectorMemory()

# ================= åŠŸèƒ½Aï¼šæ¥æ”¶pdfï¼Œllmæå–æ‘˜è¦ï¼Œå­˜å…¥å‘é‡åº“ =================
async def process_paper_upload(
    user_id: int, 
    idea_id: int, 
    file: UploadFile, 
    db: Session
):
    """
    ä¸šåŠ¡é€»è¾‘ï¼šä¸Šä¼  PDF -> è§£ææ–‡æœ¬ -> å­˜ SQL -> å­˜å‘é‡åº“
    """
    
    # 1. è¯»å– PDF å†…å®¹ (I/O æ“ä½œ)
    # UploadFile æ˜¯ FastAPI çš„ç‰¹æœ‰ç±»å‹ï¼Œç±»ä¼¼äºä¸€ä¸ªæ‰“å¼€çš„æ–‡ä»¶å¥æŸ„
    content = await file.read() 
    
    # ä¸ºäº†ç”¨ pypdf è¯»å–ï¼Œæˆ‘ä»¬éœ€è¦æŠŠäºŒè¿›åˆ¶å­˜æˆä¸´æ—¶æ–‡ä»¶ï¼Œæˆ–è€…ç”¨ BytesIO
    # è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæˆ‘ä»¬å‡è®¾æ˜¯ä¸€ä¸ªæ ‡å‡†çš„æ–‡æœ¬æå–æµç¨‹
    import io
    pdf_file = io.BytesIO(content) # æŠŠäºŒè¿›åˆ¶æµå˜æˆåƒæ–‡ä»¶ä¸€æ ·å¯è¯»çš„å¯¹è±¡
    reader = PdfReader(pdf_file)
    
    full_text = ""
    for i,page in enumerate(reader.pages):
        if i >10:break
        text = page.extract_text()
        if text:
            full_text += page.extract_text() + "\n"
    
    # 2. ä½¿ç”¨deepseekç”Ÿæˆæ‘˜è¦
    print("ä½¿ç”¨deepseekç”Ÿæˆæ‘˜è¦ä¸­...")
    structure_prompt = """
    ä½ æ˜¯ä¸€ä¸ªç§‘ç ”è®ºæ–‡åˆ†æå¸ˆã€‚è¯·åˆ†æè¿™ç¯‡è®ºæ–‡ï¼Œè¾“å‡ºçº¯ JSON å¯¹è±¡ï¼š
    {
        "summary": "300å­—ä¸­æ–‡æ‘˜è¦",
        "claims": ["æ ¸å¿ƒè´¡çŒ®1", "æ ¸å¿ƒè´¡çŒ®2"],
        "critiques": ["æŒ‡å‡ºçš„ç°æœ‰æ–¹æ³•ç¼ºé™·", "æœ¬è®ºæ–‡æ–¹æ³•çš„å±€é™æ€§", "åç›´è§‰çš„å®éªŒç»“æœ"]
    }
    """
    try:
        response = client.chat.completions.create(
            model = 'deepseek-chat',
            messages=[
                {'role':'system','content': structure_prompt},
                {'role':'user','content':full_text[:2000]}# å‘é€å‰2000å­—ç¬¦ï¼Œé˜²æ­¢è¶…é•¿
            ],
            stream=False
        )
        # è§£æ JSON (å¢åŠ å®¹é”™)
        import json
        content_str = response.choices[0].message.content.replace("```json", "").replace("```", "").strip()
        analysis = json.loads(content_str)
        
        abstract = analysis.get("summary", "æ‘˜è¦ç”Ÿæˆå¤±è´¥")
        critiques = analysis.get("critiques", [])
        if abstract == "æ‘˜è¦ç”Ÿæˆå¤±è´¥":
            print("deepseekç”Ÿæˆæ‘˜è¦å¤±è´¥...")
        else:
            print("deepseekæˆåŠŸç”Ÿæˆæ‘˜è¦ï¼")

    except Exception as e:
        print("deepseekæ€»ç»“å¤±è´¥ï¼Œé‡‡ç”¨å…¶ä»–æ–¹æ¡ˆï¼š{e}")
        #æå–å‰500å­—ä½œä¸ºæ‘˜è¦
        abstract = full_text[:500] + "..."

    title = file.filename # æš‚æ—¶ç”¨æ–‡ä»¶åå½“æ ‡é¢˜

    # 3. è°ƒç”¨ CRUD å±‚ï¼šå­˜å…¥ SQL æ•°æ®åº“
    # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†ä¿è¯æ— è®ºå‘é‡åº“æŒ‚æ²¡æŒ‚ï¼Œæˆ‘ä»¬çš„åŸºç¡€æ•°æ®éƒ½åœ¨
    paper_schema = schemas.PaperCreate(title=title, abstract=abstract, idea_id=idea_id)
    db_paper = crud.create_paper_record(db=db, paper=paper_schema, user_id=user_id)

    # 4. è°ƒç”¨ VectorMemoryï¼šå­˜å…¥å‘é‡æ•°æ®åº“
    # æˆ‘ä»¬æŠŠ paper_id å­˜è¿›å»ï¼Œè¿™æ ·ä»¥åæ£€ç´¢åˆ°å‘é‡ï¼Œèƒ½åå‘æŸ¥åˆ° SQL é‡Œçš„å®Œæ•´ä¿¡æ¯
    # ä¸ºäº† **å¯¹æŠ—æ€§æ£€ç´¢** æˆ‘ä»¬å­˜å…¥æ‘˜è¦çš„åŒæ—¶ï¼Œå­˜å…¥æ‰¹é©³
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    metadata_true = {
        "role": "paper_summary",
        "user_id": user_id,
        "idea_id": idea_id,
        "paper_db_id": db_paper.id, # å…³é”®ï¼šå»ºç«‹ SQL å’Œ Vector çš„è”ç³»
        "timestamp": current_time 
    }

    metadata_false={
                "role": "paper_critique", # ğŸ‘ˆ å…³é”®æ ‡ç­¾
                "user_id": user_id,
                "idea_id": idea_id,
                "paper_db_id": db_paper.id,
                "timestamp": current_time 
    }
    
    # æ„é€ å­˜å…¥å‘é‡åº“çš„æ–‡æœ¬
    # ---- æ­£å‘å­˜å‚¨ æŠŠæ‘˜è¦å’Œé¢˜ç›®å­˜åœ¨ä¸€èµ· ----
    save_text = f'è®ºæ–‡æ ‡é¢˜ï¼š{title}\nAIæ‘˜è¦:{abstract}\n'
    memory_core.add_memory(
        text=save_text, 
        metadata=metadata_true,
        mem_id=f"paper_{db_paper.id}"
    )

    # ---- åå‘å­˜å‚¨ å­˜å…¥æ‰¹é©³ ----
    if critiques:
        critique_text = f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\nå±€é™ä¸åæ€ï¼š{'; '.join(critiques)}"
        memory_core.add_memory(
            text=critique_text,
            metadata=metadata_false
        )


    print("è®ºæ–‡æˆåŠŸå­˜å…¥ï¼")

    return db_paper

# ================= åŠŸèƒ½B (é‡æ„ç‰ˆ)ï¼šé€šç”¨æ™ºèƒ½å¯¹è¯æµæ°´çº¿ =================
async def chat_with_deepseek(
    db: Session, 
    request: schemas.ChatRequest
):
    # --- 1. å…ˆæŠŠç”¨æˆ·çš„è¿™ä¸€å¥ï¼Œå­˜å…¥ SQL (ä¸ç®¡æ˜¯å•¥æ¨¡å¼) ---
    user_msg = models.Message(
        content=request.query,
        role="user",
        user_id=request.user_id,
        idea_id=request.idea_id # å¦‚æœæ²¡é€‰ ideaï¼Œè¿™é‡Œå°±æ˜¯ None
    )
    db.add(user_msg)
    db.commit() # æ‹¿åˆ° user_msg.id
    
    # --- 2. å‡†å¤‡ä¸Šä¸‹æ–‡ (å†å²è®°å½• + çŸ¥è¯†æ£€ç´¢) ---
    # A. è·å–æœ€è¿‘ N è½®å¯¹è¯å†å² (Context)
    history_context = ""
    if request.idea_id and request.history_len > 0:
        # æŸ¥è¿™ä¸ª Idea ä¸‹æœ€è¿‘çš„ N æ¡æ¶ˆæ¯
        last_msgs = db.query(models.Message)\
            .filter(models.Message.idea_id == request.idea_id)\
            .order_by(models.Message.created_at.desc())\
            .limit(request.history_len).all()
        
        # å€’åºå›æ¥ï¼Œå˜æˆæ—¶é—´æ­£åº
        last_msgs.reverse()
        history_context = "\n".join([f"{m.role}: {m.content}" for m in last_msgs])

    # B. Function Calling è¿™é‡Œçš„â€œFunctionâ€å°±æ˜¯å»å‘é‡åº“æŸ¥çŸ¥è¯† (RAG)
    # ä¸ç®¡ä»€ä¹ˆæ¨¡å¼ï¼Œå…ˆå»å¤§è„‘(VectorDB)é‡Œæœä¸€ä¸‹ï¼Œä»¥é˜²ç”¨æˆ·åœ¨é—®ç›¸å…³çŸ¥è¯†
    search_results = memory_core.search_memory(request.query, n_results=3)
    knowledge_context = "\n".join([f"- {r['content']}" for r in search_results])
    
    # --- 3. ç»„è£… Prompt (æ ¹æ® Mode åˆ‡æ¢ç³»ç»Ÿäººè®¾) ---
    
    # é»˜è®¤äººè®¾
    system_instruction = "ä½ æ˜¯ä¸€ä¸ªç§‘ç ”åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†å›ç­”ç”¨æˆ·ã€‚"
    
    # è·å–å½“å‰ Idea çš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    current_idea_text = "ç”¨æˆ·æš‚æ—  Idea"
    if request.idea_id:
        idea = db.query(models.Idea).filter(models.Idea.id == request.idea_id).first()
        if idea:
            current_idea_text = f"æ ‡é¢˜ï¼š{idea.title}\nè¯¦æƒ…ï¼š{idea.description}"

    # === å…³é”®ï¼šæ¨¡å¼è·¯ç”± ===
    if request.mode == 'update':
        # ã€åŠŸèƒ½ Callingã€‘: æ›´æ–°æ¨¡å¼
        system_instruction = f"""
        ä½ æ˜¯ä¸€ä¸ªç§‘ç ”Ideaè¿­ä»£ä¸“å®¶ã€‚
        ç”¨æˆ·çš„æ„å›¾æ˜¯ï¼š**ä¿®æ”¹æˆ–å®Œå–„å½“å‰çš„ Idea**ã€‚
        
        ã€å½“å‰ Ideaã€‘:
        {current_idea_text}
        
        ã€æ£€ç´¢åˆ°çš„ç›¸å…³çŸ¥è¯†/è®ºæ–‡ã€‘:
        {knowledge_context}
        
        è¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
        1. ç»“åˆç”¨æˆ·çš„æ–°æŒ‡ä»¤å’Œæ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼Œæ€è€ƒå¦‚ä½•æ”¹è¿› Ideaã€‚
        2. ç”¨è‡ªç„¶è¯­è¨€å‘ç”¨æˆ·è§£é‡Šä½ ä¿®æ”¹äº†å“ªé‡Œï¼Œä¸ºä»€ä¹ˆè¦æ”¹ã€‚
        3. **é‡è¦**ï¼šæœ€åå¿…é¡»ç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„ Idea ç‰ˆæœ¬ï¼Œå¹¶ç”¨ XML æ ‡ç­¾åŒ…è£¹ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
           <SUGGEST_IDEA>
           (è¿™é‡Œæ˜¯ä¿®æ”¹åçš„å®Œæ•´ Idea æè¿°ï¼Œä¸è¦åŒ…å«åŸæ¥çš„æ ‡é¢˜ï¼Œåªå†™æè¿°å†…å®¹)
           </SUGGEST_IDEA>
        """
        
    elif request.mode == 'critique':
        # ã€åŠŸèƒ½ Callingã€‘: æ‰¹åˆ¤æ¨¡å¼
        system_instruction = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸¥å‰çš„å®¡ç¨¿äºº (Reviewer 2)ã€‚
        è¯·åŸºäºã€æ£€ç´¢åˆ°çš„çŸ¥è¯†ã€‘ï¼š
        {knowledge_context}
        
        å¯¹ç”¨æˆ·çš„ Idea ({current_idea_text}) è¿›è¡Œæ‰¹åˆ¤ã€‚
        ä½ éœ€è¦æŒ‡å‡ºé€»è¾‘æ¼æ´ã€åˆ›æ–°ç‚¹ä¸è¶³æˆ–ä¸ç°æœ‰æ–‡çŒ®å†²çªçš„åœ°æ–¹ã€‚
        """

    else: # mode == 'chat'
        system_instruction = f"""
        ä½ æ˜¯ä¸€ä¸ªç§‘ç ”åŠ©æ‰‹ã€‚
        ã€ç›¸å…³å¯¹è¯å†å²ã€‘:
        {history_context}
        
        ã€ç›¸å…³çŸ¥è¯†åº“ã€‘:
        {knowledge_context}
        
        å¦‚æœç”¨æˆ·çš„é—®é¢˜å’Œç§‘ç ”æ— å…³ï¼Œè¯·æ­£å¸¸èŠå¤©ã€‚
        å¦‚æœç”¨æˆ·ä¼¼ä¹åœ¨æš—ç¤ºè¦ä¿®æ”¹ Ideaï¼Œè¯·æç¤ºç”¨æˆ·åˆ‡æ¢åˆ°â€œä¿®æ”¹æ¨¡å¼â€ã€‚
        """

    # --- 4. è°ƒç”¨ LLM ---
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": request.query}
            ],
            stream=False
        )
        ai_content = response.choices[0].message.content

        # --- 5. è§£æç»“æœ (Function Result Parsing) ---
        suggested_idea = None
        
        # åªæœ‰åœ¨ update æ¨¡å¼ä¸‹ï¼Œæ‰å»å°è¯•æå–æ–° Idea
        if request.mode == 'update':
            import re
            match = re.search(r"<SUGGEST_IDEA>(.*?)</SUGGEST_IDEA>", ai_content, re.DOTALL)
            if match:
                suggested_idea = match.group(1).strip()
                # æŠŠæ ‡ç­¾å»æ‰ï¼Œå‰©ä¸‹çš„ä½œä¸ºå¯¹è¯å†…å®¹è¿”å›ï¼Œæˆ–è€…ä½ å¯ä»¥ä¿ç•™æ ‡ç­¾è®©å‰ç«¯å¤„ç†
                # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©åœ¨å¯¹è¯æ–‡æœ¬é‡Œéšè—æ‰é‚£æ®µå†—é•¿çš„å®šä¹‰ï¼Œåªç•™è§£é‡Š
                ai_content = ai_content.replace(match.group(0), "\n\n(å·²ä¸ºæ‚¨ç”Ÿæˆä¿®æ”¹å»ºè®®ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹å¡ç‰‡ğŸ‘‡)")

        # --- 6. æŠŠ AI çš„å›å¤ä¹Ÿå­˜å…¥ SQL ---
        ai_msg = models.Message(
            content=ai_content, # è¿™é‡Œå­˜çš„æ˜¯å»æ‰äº† XML çš„çº¯æ–‡æœ¬
            role="ai",
            user_id=request.user_id,
            idea_id=request.idea_id
        )
        db.add(ai_msg)
        db.commit()

        # --- 7.  æ ¹æ®ç”¨æˆ·é€‰æ‹©å­˜å‚¨å¯¹è¯ä½œä¸ºçŸ¥è¯† ---
        # å¦‚æœç”¨æˆ·åœ¨å‰å°å‹¾é€‰äº† "ä½œä¸ºçŸ¥è¯†ä¿å­˜" (save_as_knowledge=True)
        # æˆ‘ä»¬å°±æŠŠè¿™è½®å¯¹è¯ä½œä¸ºâ€œé«˜æƒé‡çŸ¥è¯†â€ç«‹å³å­˜å…¥å‘é‡åº“
        if request.save_as_knowledge:
            # å­˜å…¥å‘é‡åº“
            memory_core.add_memory(
                text=f"ã€ç”¨æˆ·ç²¾é€‰çŸ¥è¯†ã€‘\né—®: {request.query}\nç­”: {ai_content}",
                metadata={
                    "user_id": request.user_id,
                    "idea_id": request.idea_id if request.idea_id else 0,
                    "role": "explicit_knowledge", # æ˜¾å¼çŸ¥è¯†æ ‡è®°
                    "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
            )
            print(f" >> å·²æ‰‹åŠ¨å›ºåŒ–çŸ¥è¯†: {request.query[:10]}...")

        # --- 8. è¿”å›ç»“æœ ---
        return schemas.ChatResponse(
            response_text=ai_content,
            suggested_idea=suggested_idea,
            used_references=[r['content'][:20] for r in search_results],
            message_id=ai_msg.id
        )

    except Exception as e:
        print(f"Chat Error: {e}")
        return schemas.ChatResponse(
            response_text="ç³»ç»Ÿå‡ºé”™äº†ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚",
            message_id=0
        )

# ================= åŠŸèƒ½Cï¼šè¿›è¡Œå¯¹æŠ—æ€§æ£€ç´¢ï¼ˆæ·±åº¦è¯„åˆ¤ï¼‰ =================  
async def critical_agent_chat(
    db: Session,
    user_id: int,
    query: str,
    idea_id: int = None
):
    """
    æ­£å‘æ£€ç´¢ + åå‘æ”»å‡» + é€»è¾‘æ‰“åˆ†ã€‚
    """
    
    # 1. åŸºç¡€ä¸Šä¸‹æ–‡æ£€ç´¢ (æ­£å‘)
    support_results = memory_core.search_memory(query, n_results=3)
    support_text = "\n".join([f"- {r['content']}" for r in support_results])

    # 2. å¯¹æŠ—æ€§æ£€ç´¢ (åå‘)
    print("æ­£åœ¨è¿›è¡Œæ‰¹åˆ¤æ€§æ€è€ƒ...")
    # ç”Ÿæˆåå‘å…³é”®è¯
    bad_keywords = utils.generate_adversarial_keywords(query)
    
    critique_evidences = []
    # å»å‘é‡åº“é‡Œä¸“é—¨æœ role='paper_critique' çš„æ•°æ®
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ vector_memory.search_memory ä»¥åå¯ä»¥æ”¯æŒ filter å‚æ•°
    # ç›®å‰å…ˆç®€å•æœå…¨æ–‡
    for kw in bad_keywords:
        res = memory_core.search_memory(kw, n_results=2)
        critique_evidences.extend(res)

    # 3. é€»è¾‘å†²çªé‡åŒ– (The Metric)
    high_conflict_points = []
    for evi in critique_evidences:
        # è°ƒç”¨ utils é‡Œçš„é‡åŒ–å™¨
        assessment = utils.calculate_conflict_score(query, evi['content'])
        
        if assessment['score'] >= 6: # åªæœ‰å†²çªåˆ†å¤§äº6çš„æ‰å€¼å¾—æŠ¥å‘Š
            high_conflict_points.append({
                "content": evi['content'],
                "score": assessment['score'],
                "reason": assessment['reason']
            })

    # 4. ç»„è£…æœ€ç»ˆ Agent Prompt
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸ä»…æä¾›å¸®åŠ©ï¼Œæ›´æä¾›â€œæ·±åº¦æ´å¯Ÿâ€çš„ç§‘ç ”ä¼™ä¼´ã€‚
    ç”¨æˆ·æ­£åœ¨æ€è€ƒï¼š"{query}"
    
    ã€å·²æœ‰æ”¯æŒè¯æ®ã€‘:
    {support_text}
    
    ã€âš ï¸ æ½œåœ¨çš„é€»è¾‘é£é™© (åŸºäºç°æœ‰è®ºæ–‡çš„åé©³)ã€‘:
    {json.dumps(high_conflict_points, ensure_ascii=False)}
    
    è¯·å›å¤ç”¨æˆ·ï¼š
    1. é¦–å…ˆè‚¯å®š Idea çš„ä»·å€¼ï¼ˆå¦‚æœæœ‰æ”¯æŒè¯æ®ï¼‰ã€‚
    2. **æ ¸å¿ƒä»»åŠ¡**ï¼šå¦‚æœå­˜åœ¨é«˜åˆ†å†²çªï¼ˆScore > 6ï¼‰ï¼Œå¿…é¡»ä¸¥è‚ƒæŒ‡å‡ºè¿™ä¸ª Idea çš„æ½œåœ¨ç¼ºé™·ã€‚ä¸è¦ä¸€å‘³èµåŒã€‚
    3. ç»¼åˆå»ºè®®ä¸‹ä¸€æ­¥è¯¥æ€ä¹ˆåšã€‚
    """

    # 5. ç”Ÿæˆæœ€ç»ˆå›å¤
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[{"role": "system", "content": system_prompt}],
        stream=False
    )
    
    return response.choices[0].message.content




