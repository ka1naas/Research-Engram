'''
åŠŸèƒ½aï¼šå¯¹ä¸Šä¼ çš„è®ºæ–‡ç”Ÿæˆæ‘˜è¦ï¼Œä»¥åŠå¯¹æŠ—æ€§æœç´¢éœ€è¦çš„å†…å®¹
åŠŸèƒ½bï¼šå¸®ç”¨æˆ·ç”Ÿæˆæˆ–æ”¹è¿›ideaï¼Œæˆ–è€…ï¼Œè¿›è¡Œæ™®é€šå¯¹è¯ï¼Œå››ç§æ¨¡å¼
åŠŸèƒ½cï¼šåœ¨ç”¨æˆ·å·²ç»æœ‰ideaçš„æƒ…å†µä¸‹ï¼Œéœ€è¦ä¸€ç‚¹æ‰¹è¯„æ—¶æ‰é‡‡ç”¨çš„
åç»­æ›´æ–°è®¡åˆ’ï¼šåŠ å…¥çœŸæ­£çš„function callingå®ç°ç®€å•çš„agentä»»åŠ¡
'''
import os
from sqlalchemy.orm import Session
from pypdf import PdfReader
from fastapi import UploadFile
import schemas, crud
from vector_memory import VectorMemory
import datetime
import openai
from dotenv import load_dotenv
import models
import utils
import json
import re

# è¯»å– .env
load_dotenv()

# åˆå§‹åŒ– DeepSeek
client = openai.OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"), # ä»ç³»ç»Ÿç¯å¢ƒå˜é‡é‡Œæ‹¿é’¥åŒ™ï¼Œè€Œä¸æ˜¯å†™æ­»åœ¨ä»£ç é‡Œ
    base_url="https://api.deepseek.com"
)

# åˆå§‹åŒ–å‘é‡è®°å¿†åº“ï¼ˆå•ä¾‹æ¨¡å¼ï¼šæ•´ä¸ªç³»ç»Ÿåªç”¨è¿™ä¸€ä¸ªå®ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹ï¼‰
# æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å‡è®¾ vector_memory.py åœ¨åŒä¸€ç›®å½•ä¸‹
memory_core = VectorMemory()

# ================= åŠŸèƒ½Aï¼šæ¥æ”¶pdfï¼Œä¿å­˜å…¨æ–‡ï¼Œllmæå–æ‘˜è¦ï¼Œå­˜å…¥å‘é‡åº“ =================
async def process_paper_upload(
    user_id: int, 
    idea_id: int, 
    file: UploadFile, 
    db: Session
):
    """
    ä¸šåŠ¡é€»è¾‘ï¼šä¸Šä¼  PDF -> è§£ææ–‡æœ¬ -> å­˜ SQL -> å­˜å‘é‡åº“
    """
    
    # 1. è¯»å– PDF å†…å®¹ (I/O æ“ä½œ)
    # UploadFile æ˜¯ FastAPI çš„ç‰¹æœ‰ç±»å‹ï¼Œç±»ä¼¼äºä¸€ä¸ªæ‰“å¼€çš„æ–‡ä»¶å¥æŸ„
    content = await file.read() 
    
    # ä¸ºäº†ç”¨ pypdf è¯»å–ï¼Œæˆ‘ä»¬éœ€è¦æŠŠäºŒè¿›åˆ¶å­˜æˆä¸´æ—¶æ–‡ä»¶ï¼Œæˆ–è€…ç”¨ BytesIO
    # è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæˆ‘ä»¬å‡è®¾æ˜¯ä¸€ä¸ªæ ‡å‡†çš„æ–‡æœ¬æå–æµç¨‹
    import io
    pdf_file = io.BytesIO(content) # æŠŠäºŒè¿›åˆ¶æµå˜æˆåƒæ–‡ä»¶ä¸€æ ·å¯è¯»çš„å¯¹è±¡
    reader = PdfReader(pdf_file)
    
    full_text = ""
    for i,page in enumerate(reader.pages):
        if i >10:break
        text = page.extract_text()
        if text:
            full_text += page.extract_text() + "\n"
    
    # 2. ä½¿ç”¨deepseekç”Ÿæˆæ‘˜è¦
    print("ä½¿ç”¨deepseekç”Ÿæˆæ‘˜è¦ä¸­...")
    structure_prompt = """
    ä½ æ˜¯ä¸€ä¸ªç§‘ç ”è®ºæ–‡åˆ†æå¸ˆã€‚è¯·åˆ†æè¿™ç¯‡è®ºæ–‡ï¼Œè¾“å‡ºçº¯ JSON å¯¹è±¡ï¼š
    {
        "summary": "300å­—ä¸­æ–‡æ‘˜è¦",
        "claims": ["æ ¸å¿ƒè´¡çŒ®1", "æ ¸å¿ƒè´¡çŒ®2"],
        "critiques": ["æŒ‡å‡ºçš„ç°æœ‰æ–¹æ³•ç¼ºé™·", "æœ¬è®ºæ–‡æ–¹æ³•çš„å±€é™æ€§", "åç›´è§‰çš„å®éªŒç»“æœ"]
    }
    """
    try:
        response = client.chat.completions.create(
            model = 'deepseek-chat',
            messages=[
                {'role':'system','content': structure_prompt},
                {'role':'user','content':full_text[:2000]}# å‘é€å‰2000å­—ç¬¦ï¼Œé˜²æ­¢è¶…é•¿
            ],
            stream=False
        )
        # è§£æ JSON (å¢åŠ å®¹é”™)
        import json
        content_str = response.choices[0].message.content.replace("```json", "").replace("```", "").strip()
        analysis = json.loads(content_str)
        
        abstract = analysis.get("summary", "æ‘˜è¦ç”Ÿæˆå¤±è´¥")
        critiques = analysis.get("critiques", [])
        if abstract == "æ‘˜è¦ç”Ÿæˆå¤±è´¥":
            print("deepseekç”Ÿæˆæ‘˜è¦å¤±è´¥...")
        else:
            print("deepseekæˆåŠŸç”Ÿæˆæ‘˜è¦ï¼")

    except Exception as e:
        print("deepseekæ€»ç»“å¤±è´¥ï¼Œé‡‡ç”¨å…¶ä»–æ–¹æ¡ˆï¼š{e}")
        #æå–å‰500å­—ä½œä¸ºæ‘˜è¦
        abstract = full_text[:500] + "..."

    title = file.filename # æš‚æ—¶ç”¨æ–‡ä»¶åå½“æ ‡é¢˜

    # 3. è°ƒç”¨ CRUD å±‚ï¼šå­˜å…¥ SQL æ•°æ®åº“
    # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†ä¿è¯æ— è®ºå‘é‡åº“æŒ‚æ²¡æŒ‚ï¼Œæˆ‘ä»¬çš„åŸºç¡€æ•°æ®éƒ½åœ¨
    paper_schema = schemas.PaperCreate(
        title=title, 
        abstract=abstract, 
        idea_id=idea_id,
        full_text=full_text # ä¼ ç»™ schema
    )
    db_paper = crud.create_paper_record(db=db, paper=paper_schema, user_id=user_id)

    # 4. è°ƒç”¨ VectorMemoryï¼šå­˜å…¥å‘é‡æ•°æ®åº“
    # æˆ‘ä»¬æŠŠ paper_id å­˜è¿›å»ï¼Œè¿™æ ·ä»¥åæ£€ç´¢åˆ°å‘é‡ï¼Œèƒ½åå‘æŸ¥åˆ° SQL é‡Œçš„å®Œæ•´ä¿¡æ¯
    # ä¸ºäº† **å¯¹æŠ—æ€§æ£€ç´¢** æˆ‘ä»¬å­˜å…¥æ‘˜è¦çš„åŒæ—¶ï¼Œå­˜å…¥æ‰¹é©³
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    metadata_true = {
        "role": "paper_summary",
        "user_id": user_id,
        "idea_id": idea_id,
        "paper_db_id": db_paper.id, # å…³é”®ï¼šå»ºç«‹ SQL å’Œ Vector çš„è”ç³»
        "timestamp": current_time 
    }

    metadata_false={
                "role": "paper_critique", # ğŸ‘ˆ å…³é”®æ ‡ç­¾
                "user_id": user_id,
                "idea_id": idea_id,
                "paper_db_id": db_paper.id,
                "timestamp": current_time 
    }
    
    # æ„é€ å­˜å…¥å‘é‡åº“çš„æ–‡æœ¬
    # ---- æ­£å‘å­˜å‚¨ æŠŠæ‘˜è¦å’Œé¢˜ç›®å­˜åœ¨ä¸€èµ· ----
    save_text = f'è®ºæ–‡æ ‡é¢˜ï¼š{title}\nAIæ‘˜è¦:{abstract}\n'
    memory_core.add_memory(
        text=save_text, 
        metadata=metadata_true,
        mem_id=f"paper_{db_paper.id}"
    )

    # ---- åå‘å­˜å‚¨ å­˜å…¥æ‰¹é©³ ----
    if critiques:
        critique_text = f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\nå±€é™ä¸åæ€ï¼š{'; '.join(critiques)}"
        memory_core.add_memory(
            text=critique_text,
            metadata=metadata_false
        )


    print("è®ºæ–‡æˆåŠŸå­˜å…¥ï¼")

    return db_paper

# ================= åŠŸèƒ½B (é‡æ„ç‰ˆ)ï¼šé€šç”¨æ™ºèƒ½å¯¹è¯æµæ°´çº¿ï¼Œå«function calling =================
# services.py

async def chat_with_deepseek(db: Session, request: schemas.ChatRequest):
    # 1. å­˜ç”¨æˆ·æ¶ˆæ¯
    user_msg = models.Message(content=request.query, role="user", user_id=request.user_id, idea_id=request.idea_id)
    db.add(user_msg)
    db.commit()

    final_answer = ""
    used_refs = []

    # ğŸŸ¢ é¢„å…ˆå®šä¹‰è¿‡æ»¤æ¡ä»¶ (å¤ç”¨é€»è¾‘)
    # é€»è¾‘ï¼šåªæœ‰å½“ (é€‰äº†Idea) ä¸” (æ²¡å¼€å…¨å±€æœç´¢) æ—¶ï¼Œæ‰é™åˆ¶èŒƒå›´
    # å¦åˆ™ (æ²¡é€‰Idea æˆ– å¼€äº†å…¨å±€) -> filter ä¸º None (æœå…¨éƒ¨)
    current_filter = {"idea_id": request.idea_id} if (request.idea_id and not request.enable_global_search) else None
    
    # ç”¨äºæ‰“å°æ—¥å¿—çœ‹çœ‹
    mode_name = "ğŸŒ å…¨å±€è”æƒ³" if not current_filter else f"ğŸ”’ ä¸“æ³¨å½“å‰(ID:{request.idea_id})"

    # ================= åˆ†æ”¯ä¸€ï¼šæŒ‡å®šäº†è®ºæ–‡ (Context Locked) =================
    if request.paper_id:
        paper = db.query(models.Paper).filter(models.Paper.id == request.paper_id).first()
        if not paper:
            return schemas.ChatResponse(response_text="âŒ æ‰¾ä¸åˆ°æŒ‡å®šçš„è®ºæ–‡æ•°æ®", message_id=0)

        # --- A. æ·±åº¦é˜…è¯»æ¨¡å¼ (Full Text) ---
        # è¿™ç§æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬è¦æ·±åº¦è¯»è¿™ä¸€ç¯‡ï¼Œé€šå¸¸ä¸éœ€è¦ RAG å¹²æ‰°ï¼Œæ‰€ä»¥ä¸ä½¿ç”¨ filter
        if request.use_full_text:
            print(f"ğŸ“– [æ·±åº¦æ¨¡å¼] é˜…è¯»å…¨æ–‡ï¼š{paper.title}")
            if not paper.full_text:
                return schemas.ChatResponse(response_text="âš ï¸ è¯¥è®ºæ–‡æœªå½•å…¥å…¨æ–‡æ•°æ®", message_id=0)

            system_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºæ–‡å®¡ç¨¿äººã€‚ç”¨æˆ·æŒ‡å®šäº†ä¸€ç¯‡è®ºæ–‡è¿›è¡Œã€æ·±åº¦ç ”è¯»ã€‘ã€‚
            ã€æ ‡é¢˜ã€‘: {paper.title}
            ã€å…¨æ–‡ã€‘:
            {paper.full_text[:35000]} 
            è¯·åŸºäºå…¨æ–‡ç»†èŠ‚å›ç­”ã€‚
            """
            messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": request.query}]

        # --- B. æ‘˜è¦èšç„¦ + RAG è”æƒ³æ¨¡å¼ ---
        # ğŸŸ¢ å…³é”®ç‚¹ï¼šè¿™é‡Œè¦ç”¨åˆ° current_filter
        else:
            print(f"ğŸ” [æ‘˜è¦æ¨¡å¼] {mode_name} - è®ºæ–‡ï¼š{paper.title}")
            
            # 1. åŸºç¡€æ˜¯æ‘˜è¦
            base_context = f"ã€å½“å‰è®¨è®ºè®ºæ–‡ã€‘\næ ‡é¢˜ï¼š{paper.title}\næ‘˜è¦ï¼š{paper.abstract}"
            
            # 2. RAG æ£€ç´¢ (è¿™é‡Œç”¨åˆ°äº† filterï¼)
            # å¦‚æœå¼€å¯å…¨å±€ï¼Œè¿™é‡Œå°±èƒ½æœåˆ°å…¶ä»– Idea çš„ç›¸å…³è®ºæ–‡
            search_results = memory_core.search_memory(
                request.query, 
                n_results=3, 
                filter_metadata=current_filter # ğŸ‘ˆ æ³¨å…¥è¿‡æ»¤é€»è¾‘
            )
            rag_context = "\n".join([f"- {r['content']}" for r in search_results])
            used_refs = [r['content'][:20] for r in search_results]

            system_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªç§‘ç ”åŠ©æ‰‹ã€‚
            {base_context}
            
            ã€å…³è”çŸ¥è¯† ({mode_name})ã€‘:
            {rag_context}
            
            è¯·ç»“åˆæ‘˜è¦å’Œå…³è”çŸ¥è¯†å›ç­”ã€‚
            """
            messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": request.query}]

        # æ‰§è¡Œ LLM (åˆ†æ”¯ä¸€)
        response = client.chat.completions.create(model="deepseek-chat", messages=messages, stream=False)
        final_answer = response.choices[0].message.content

    # ================= åˆ†æ”¯äºŒï¼šAgent è‡ªç”±æ¨¡å¼ (æ— æŒ‡å®š Paper) =================
    else:
        print(f"ğŸ¤– [Agentæ¨¡å¼] {mode_name}")
        
        # 1. å†å²è®°å½•
        history_context = ""
        if request.idea_id and request.history_len > 0:
            # å†å²è®°å½•ä¾ç„¶å»ºè®®åªçœ‹å½“å‰çš„ï¼Œå¦åˆ™å¯¹è¯å¤ªä¹±ã€‚
            # å½“ç„¶ï¼Œå¦‚æœä½ æƒ³è®©â€œå¯¹è¯å†å²â€ä¹Ÿè·¨ Ideaï¼Œå¯ä»¥æŠŠ filter å»æ‰ã€‚è¿™é‡Œæš‚ä¸”ä¿æŒåªçœ‹å½“å‰ Idea çš„å†å²ã€‚
            last_msgs = db.query(models.Message).filter(models.Message.idea_id == request.idea_id).order_by(models.Message.created_at.desc()).limit(request.history_len).all()
            last_msgs.reverse()
            history_context = "\n".join([f"{m.role}: {m.content}" for m in last_msgs])

        # 2. Agent æ€è€ƒ
        agent_system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªç§‘ç ”åŠ©æ‰‹ã€‚
        è§„åˆ™ï¼š
        1. éœ€è¦æŸ¥èµ„æ–™ -> è¾“å‡º <TOOL_CALL>search: å…³é”®è¯</TOOL_CALL>
        2. å¦åˆ™ -> ç›´æ¥å›ç­”ã€‚
        """

        resp1 = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": agent_system_prompt},
                {"role": "user", "content": f"å†å²:\n{history_context}\né—®é¢˜:\n{request.query}"}
            ]
        )
        first_content = resp1.choices[0].message.content
        
        # 3. å·¥å…·æ£€æµ‹ä¸æ‰§è¡Œ
        tool_query = detect_tool_call(first_content)
        
        if tool_query:
            keyword = tool_query.replace("search:", "").strip()
            print(f"ğŸ”§ Agent æ­£åœ¨æœç´¢: {keyword} | æ¨¡å¼: {mode_name}")
            
            # ğŸŸ¢ å…³é”®ç‚¹ï¼šAgent æœç´¢æ—¶ä¹Ÿè¦éµå®ˆ filter è§„åˆ™
            res = memory_core.search_memory(
                keyword, 
                n_results=3, 
                filter_metadata=current_filter # ğŸ‘ˆ æ³¨å…¥è¿‡æ»¤é€»è¾‘
            )
            
            knowledge = "\n".join([f"- {r['content']}" for r in res])
            used_refs = [r['content'][:20] for r in res]
            
            resp2 = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ç»“åˆæ£€ç´¢ç»“æœå›ç­”ï¼š"},
                    {"role": "user", "content": f"é—®é¢˜:{request.query}\nèµ„æ–™:{knowledge}"}
                ]
            )
            final_answer = resp2.choices[0].message.content
        else:
            final_answer = first_content

    # ================= æ”¶å°¾ =================
    ai_msg = models.Message(content=final_answer, role="ai", user_id=request.user_id, idea_id=request.idea_id)
    db.add(ai_msg)
    db.commit()

    return schemas.ChatResponse(
        response_text=final_answer,
        suggested_idea=None,
        used_references=used_refs,
        message_id=ai_msg.id
    )

# ================= åŠŸèƒ½Cï¼šè¿›è¡Œå¯¹æŠ—æ€§æ£€ç´¢ï¼ˆæ·±åº¦è¯„åˆ¤ï¼‰ =================  
async def critical_agent_chat(
    db: Session,
    user_id: int,
    query: str,
    idea_id: int = None
):
    """
    æ­£å‘æ£€ç´¢ + åå‘æ”»å‡» + é€»è¾‘æ‰“åˆ†ã€‚
    """
    
    # 1. åŸºç¡€ä¸Šä¸‹æ–‡æ£€ç´¢ (æ­£å‘)
    support_results = memory_core.search_memory(query, n_results=3)
    support_text = "\n".join([f"- {r['content']}" for r in support_results])

    # 2. å¯¹æŠ—æ€§æ£€ç´¢ (åå‘)
    print("æ­£åœ¨è¿›è¡Œæ‰¹åˆ¤æ€§æ€è€ƒ...")
    # ç”Ÿæˆåå‘å…³é”®è¯
    bad_keywords = utils.generate_adversarial_keywords(query)
    
    critique_evidences = []
    # å»å‘é‡åº“é‡Œä¸“é—¨æœ role='paper_critique' çš„æ•°æ®
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ vector_memory.search_memory ä»¥åå¯ä»¥æ”¯æŒ filter å‚æ•°
    # ç›®å‰å…ˆç®€å•æœå…¨æ–‡
    for kw in bad_keywords:
        res = memory_core.search_memory(kw, n_results=2)
        critique_evidences.extend(res)

    # 3. é€»è¾‘å†²çªé‡åŒ– (The Metric)
    high_conflict_points = []
    for evi in critique_evidences:
        # è°ƒç”¨ utils é‡Œçš„é‡åŒ–å™¨
        assessment = utils.calculate_conflict_score(query, evi['content'])
        
        if assessment['score'] >= 6: # åªæœ‰å†²çªåˆ†å¤§äº6çš„æ‰å€¼å¾—æŠ¥å‘Š
            high_conflict_points.append({
                "content": evi['content'],
                "score": assessment['score'],
                "reason": assessment['reason']
            })

    # 4. ç»„è£…æœ€ç»ˆ Agent Prompt
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸ä»…æä¾›å¸®åŠ©ï¼Œæ›´æä¾›â€œæ·±åº¦æ´å¯Ÿâ€çš„ç§‘ç ”ä¼™ä¼´ã€‚
    ç”¨æˆ·æ­£åœ¨æ€è€ƒï¼š"{query}"
    
    ã€å·²æœ‰æ”¯æŒè¯æ®ã€‘:
    {support_text}
    
    ã€âš ï¸ æ½œåœ¨çš„é€»è¾‘é£é™© (åŸºäºç°æœ‰è®ºæ–‡çš„åé©³)ã€‘:
    {json.dumps(high_conflict_points, ensure_ascii=False)}
    
    è¯·å›å¤ç”¨æˆ·ï¼š
    1. é¦–å…ˆè‚¯å®š Idea çš„ä»·å€¼ï¼ˆå¦‚æœæœ‰æ”¯æŒè¯æ®ï¼‰ã€‚
    2. **æ ¸å¿ƒä»»åŠ¡**ï¼šå¦‚æœå­˜åœ¨é«˜åˆ†å†²çªï¼ˆScore > 6ï¼‰ï¼Œå¿…é¡»ä¸¥è‚ƒæŒ‡å‡ºè¿™ä¸ª Idea çš„æ½œåœ¨ç¼ºé™·ã€‚ä¸è¦ä¸€å‘³èµåŒã€‚
    3. ç»¼åˆå»ºè®®ä¸‹ä¸€æ­¥è¯¥æ€ä¹ˆåšã€‚
    """

    # 5. ç”Ÿæˆæœ€ç»ˆå›å¤
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[{"role": "system", "content": system_prompt}],
        stream=False
    )
    
    return response.choices[0].message.content




